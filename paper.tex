% Template for ICASSP-2017 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}


\usepackage[final]{nips_2016}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[draft]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath,graphicx}
\usepackage{bm}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm2e}
\usepackage{pbox}
\usepackage{multirow}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Invariant Representations for Noisy Speech Recognition}
%
% Single address.
% ---------------
%\name{Dmitriy Serdyuk $^{\dagger}$\thanks{Dmitriy Serdyuk performed the work
%  while at IBM}, Kartik Audhkhasi$^{\star}$, Phil\'emon Brakel$^{\dagger}$, Bhuvana Ramabhadran$^{\star}$, Yoshua Bengio$^{\dagger}$\thanks{Yoshua Bengio is a CIFAR Fellow.}}
%\address{$^{\dagger}$ Universit\'e de Montr\'eal\\
%         $^{\star}$ IBM}
\author{
  Dmitriy Serdyuk\thanks{Dmitriy Serdyuk performed the work 
    during an internship at IBM Watson.} \\
  MILA\\
  Universit\'e de Montr\'eal\\
  Montr\'eal, QC H3T 1J4 \\
  \texttt{serdyuk@iro.umontreal.ca} \\
  \And
  Kartik Audhkhasi \\
  IBM Watson \\
  Yorktown Heights, NY 10598\\
  \texttt{kaudhkha@us.ibm.com} \\
  \And
  Phil\'emon Brakel \\
  MILA\\
  Universit\'e de Montr\'eal\\
  Montr\'eal, QC H3T 1J4 \\
  \texttt{pbpop3@gmail.com} \\
  \And
  Bhuvana Ramabhadran\\
  IBM Watson\\
  Yorktown Heights, NY 10598\\
  \texttt{bhuvana@us.ibm.com} \\
  \And
  Samuel Thomas\\
  IBM Watson\\
  Yorktown Heights, NY 10598\\
  \texttt{samthom@us.ibm.com} \\
  \And
  Yoshua Bengio\\
  MILA, CIFAR Fellow\\
  Universit\'e de Montr\'eal\\
  Montr\'eal, QC H3T 1J4 \\
  \texttt{findme@inter.net} \\
}



\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
    Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations.
    We use ideas from recent research on image generation using
    Generative Adversarial Networks and domain adaptation ideas extending
    adversarial gradient-based training. A recent work from Ganin et al. proposes to
    use adversarial training for image domain adaptation by using an intermediate
    representation from the main target classification network to deteriorate the domain 
    classifier performance through a separate neural network.
    Our work focuses on investigating neural architectures which produce
    representations invariant to noise conditions for ASR.  We
    experimentally evaluate the proposed architecture on a Aurora-4, a popular benchmark for
    noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.
\end{abstract}
%
%\begin{keywords}
%speech recognition, deep neural networks, invariant representations, adversarial training
%\end{keywords}
%
\section{Introduction}
\label{sec:intro}
    One of the most challenging aspects of automatic speech recognition (ASR)
    is the mismatch between the training and testing acoustic conditions. During
    testing, a system may encounter new recording conditions, microphone types, speakers,
    accents and types of background noises. Even in case all testing conditions have also appeared
    in the training data, their acoustic statistics likely to be
    different. For this reason, it's important to develop ASR systems that are
    robust towards such undesirable acoustic variabilities.

    While many types of model and feature based adaptation methods like maximum likelihood linear regression (MLLR) and feature-space MLLR (fMLLR)
    \cite{leggetter1995maximum} have been proposed, most of these methods were
    designed with Gaussian mixture model-hidden Markov model (GMM-HMM) systems in mind. Since the enormous success of
    Deep Neural Network (DNN) acoustic models for ASR \cite{hinton2012deep}, GMM-based
    model adaptation and feature-space adaptation are not optimal for the overall hybrid DNN-HMM system.
    Moreover, the discriminative gradient-based optimization used to train
    DNNs, in combination with the ability of DNNs to learn highly non-linear
    feature transformations, allows for greater flexibility in
    constructing training objective functions that promote learning of noise invariant
    representations.

    The main idea of this work is that instead of explicitly using noise robust acoustic features,
    we force the acoustic model to learn a representation invariant to noise 
    conditions. This training requires the noise condition labels labels in the training dataset
    , though, the 
    test data does not need to contain these labels. Since this work is tightly connected to the idea
    of generative adversarial networks (GAN) proposed 
    in~\cite{goodfellow2014generative}, Section~\ref{sec:gans} presents an overview of GANs. Section~\ref{sec:gradient-reverse} then presents the gradient reverse method proposed in
    the context of image classification in~\cite{ganin2014unsupervised}. We then present the application 
    of this method to speech recognition
    in Section~\ref{sec:invariant-speech}. Section~\ref{sec:experiments} presents the development of an acoustic model learning invariant
    representations constructed similarly to the gradient reverse method
    from Section~\ref{sec:invariant-speech}. This section presents thorough experimentation
    on a publicly available Aurora-4 dataset for noisy speech recognition.

\section{Generative Adversarial Networks}
\label{sec:gans}
    One of the most insightful recent developments in the area of generative models
    is the family of generative adversarial networks (GANs).
    A GAN consists of two networks: generator and discriminator. The generator 
    network $G$ has an
    input of randomly-generated feature vectors and is asked to produce data, e.g. an image, 
    similar to the images in the training set. The discriminator network $D$
    can either receive a generated image from the generator $G$ or an image
    from the training set. Its task is to distinguish
    between the ``fake'' generated image and the ``real'' image taken from the dataset. Thus,
    the discriminator is just a classifier network with a sigmoid output neuron
    and can be trained with gradient backpropagation. 

    The main idea of generative adversarial
    training is to stack the discriminator and generator network on top of each other and propagate the gradient of the discriminator
    network further to the generator network. In other words, these two networks are competing with each other. The 
    generator is trying to deceive the discriminator network, while the discriminator tries
    to do its best to recognize if there was a deception, similar to adversarial game-theoretic settings.    
    Formally, the objective function of GAN training is
    \begin{align*}
        \min_G \max_D V(D, G) = \mathbb{E}_{\bm{x} \sim p_{\text{data}}(\bm{x})}[\log D(\bm{x})] + 
            \mathbb{E}_{\bm{z} \sim p_{\bm{z}}(\bm{z})}[\log (1 - D(G(\bm{z})))].
    \end{align*}
    The maximization over the discriminator $D$ forms a usual cross-entropy objective, the gradients are
    computed with respect to the parameters of $D$. The parameters of $G$ are minimized using the gradients
    propagated through the second term. The minimization over $G$ makes it to produce examples which $D$
    classifiers as the training ones.
	%{\bf Please explain how the min-max applies to the two terms. I assume that we are maximizing first term with respect to D and doing min over D/max over G for the second term. Hence maximizing the first term increases the likelihood of the discriminator output (single sigmoid neuron) for true training data. Maximizing the second term decreases the likelihood of the discriminator output for fake training data. But then, what does the min over G mean? That you are maximizing the generator likelihood of the fake data?}
    % Dima: deleted paragraph and explained min-max
    
\section{Gradient Reverse Method}
\label{sec:gradient-reverse}

    %{\bf We need to write a couple of sentences on how this is motivated by GAN.} 
    %Dima: added a sentence at the end of this paragraph
    Prior work by~\cite{ganin2014unsupervised} proposed a method of training a network 
    which can be adapted to new domains. The training set consists of the images
    labeled with classes of interest and separate domain labels. For example, the domain may refer to image background. The testing set does not need
    to have the domain labels. The network has a fork-like structure: the image is fed to the
    first network which produces a hidden representation $h$. Then this representation $h$ is input to two separate networks: a domain classifier network (D) and 
    a target classifier network (R). The goal of training is to learn the hidden 
    representation that is invariant to the domain labels and performs well on the target classification task, so that the domain information doesn't 
    interfere with the target classifier at test time. Like the GAN objective forces the generation distribution be close to the data distribution,
    the gradient reverse method makes domain distributions similar to each other.

    The network is trained with three goals: the hidden representation $h$ should
    be helpful for the target classifier, harmful for the domain classifier,
    and the domain classifier should have a good classification accuracy. More 
    formally, the authors define the loss function as
    \begin{equation}
        L = L(\hat{y}, y; \theta_R, \theta_E) + 
        \alpha L(\hat{d}, d; \theta_D) -
        \beta L(\hat{d}, d; \theta_E),
        \label{eq:grm}
    \end{equation}
    where $y$ is the ground truth class, $d$ is the domain label, corresponding
    hat variables are the network predictions, and $\theta_E, \theta_R$ and $\theta_D$ are the subsets of  parameters for the encoder,
    recognizer and the domain classifier networks respectively. The hyper-parameters
    $\alpha$ and $\beta$ denote the relative influence of the loss functions terms.

\subsection{Invariant Representations for Speech Recognition}
\label{sec:invariant-speech}

This work focuses on speech recognition using DNN-HMM hybrid systems. The context dependent (CD) HMM states are the class labels of interest. The
recording conditions, speaker identity, or gender may be considered to be domains. Then the task is to make the hidden layer representations of the CD state classifier network 
invariant with respect to these domains. We hypothesize that this adversarial method of
training helps the CD state classifier to generalize better to unseen domain conditions and requires only a  
small additional amount of supervision, i.e. the domain labels.  

Figure~\ref{fig:model} depicts the model, which is same as the model for the gradient reverse method presented in the previous section. It is a feed-forward neural network trained to predict the CD HMM state, with a branch that predicts the domain (noise condition). This branch is discarded in the testing phase. In our experiments we
used the noise condition as the domain label merging all noise types into one label
and clean as the other label. Our training loss function is
    \begin{equation}
        L = L_1(\hat{y}, y; \theta_R, \theta_E) + 
        L_2(\hat{d}, d; \theta_D) +
        \beta L_3(\hat{d}, d; \theta_E),
        \label{eq:cost}
    \end{equation}
where $L_3 = - d\log(1 - \hat{d}) - (1-d)\log(\hat{d})$ maximizes the probability
of an incorrect domain classification in contrast to the Eq.~\ref{eq:grm} where the 
correct classification is minimized.
%{\bf Contrast this with term 3 of the original gradient reverse equation.}. 
The terms $L_1$ and $L_2$ are 
regular cross-entropies which are minimized with corresponding parameters $\theta_E$ and $\theta_D$.
For simplicity, we use only a single hyper-parameter -- the weight of the third term.


\begin{figure}
    \centering
    \captionsetup[subfigure]{oneside,margin={0.3cm,0cm}}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{model.pdf}
        \caption{The model consists of three neural networks. The encoder $E$ produces
        the intermediate representation $h$ which used in the recognizer $R$ and 
        in the domain discriminator $D$. The hidden representation $h$ is trained to improve
        the recognition and minimize the domain discriminator accuracy. The domain discriminator
        is a classifier trained to maximize its accuracy on the noise type
        classification task.}
        \label{fig:model}
    \end{subfigure}%
    \begin{subfigure}[b]{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{wer_avg.pdf}
        \includegraphics[width=\linewidth]{wer_seen_unseen.pdf}
        \caption{Up: Average performance of the baseline multi-condition and invariance model varying with  the number of noise
            conditions used for training. Bottom: Average performance on seen versus unseen noise conditions.
            Testing was performed on all wv1 conditions.
            }
        \label{fig:results}
    \end{subfigure}
    \caption{Model structure for invariant training and ASR results.}
\end{figure}

\section{Related Work}
    Generative adversarial networks were proposed and experimentally evaluated 
    in~\cite{goodfellow2014generative}. This work was continued 
    in work of~\cite{radford2015unsupervised}, which proposes several practical guidelines
    for training GANs. This direction is further explored in~\cite{salimans2016improved}.

    The conditioned GAN for generation task were utilized in~\cite{denton2015deep}. This
    work proposes to improve images increasing the resolution step-by-step. 
    The domain adaptation with the gradient reverse method was proposed in~\cite{ganin2014unsupervised}.

    %TODO: cite denoising papers? some work on aurora 4?
    
    The influence of representations produced by a neural network to internal noise reduction 
    is discussed in~\cite{yu2013feature} and this work sets a baseline for experiments on
    Aurora-4 dataset.

    In a work~\cite{yusuke2016adversarial} an acoustic model: multilayer sigmoidal network is 
    trained in an adversarial fashion for a task of dataset corrupted with additive noise.
    
    %{\bf Need to call out differences from this work: (1) We use the standard Aurora-4 training/testing conditions. (2) We also show the impact of seeing different number of noise types during training. (3) ...}

\section{Experiments}
\label{sec:experiments}
We experimentally evaluate the approach discussed in Section~\ref{sec:gradient-reverse} 
on the Aurora-4~\cite{parihar2002aurora} noisy speech recognition dataset. Aurora-4
is based on the Wall Street Journal corpus (WSJ0). It contains noises of 
six categories which was added to clean
data. Every clean and noisy utterance is filtered to simulate the frequency characteristics of the sampling frequency. 
% Dima: is it correct now?
%
The training
set contains approximately 4400 clean utterances and 446 utterances of each noise condition,
i.e. a total of 2676 noisy utterances.
The testing
set contains clean data, data corrupted by the 6 noises, and data recorded with a different microphone
for both clean and noisy cases.

We extracted 40-dimensional Mel-filterbank features with their deltas and 
delta-deltas spliced over $\pm$5 frames, resulting in 1320 input 
features. We performed speaker-independent global mean and variance normalization on these features. We trained 6 layer
DNN with 2048 rectified linear units at every layer using momentum-accelerated stochastic gradient descent for 15 epochs. We annealed the learning rate according 
to the development set cross-entropy, dividing it by 2 when the cross entropy did not improve.

In order to evaluate the impact of our method on generalization to unseen noises,
we performed 6 experiments with different set of seen noises. We always train
on clean data and add noise conditions one-by-one in the following order: airport, babble, car, 
restaurant, street, train. The last training group includes all noises therefore matches the
standard multi-condition training setup. For every group of experiments we trained the
baseline and the invariance model where we branch out at the $4^{th}$ layer to an  
binary classifier predicting clean versus noisy data. Due to the imbalance between amounts of clean and
noisy utterances, we had to oversample noisy frames. Hence every mini-batch contained
equal number of clean and noisy speech frames.

Table~\ref{tab:results} summarizes the results. Figure~\ref{fig:results} visualizes the word error rate for the baseline multi-condition training and invariance training as the number of seen noise types varies. We conclude that the best performance
gain is achieved when a small number of noise types are available during training. We conclude that invariance training is able to generalize better to unseen noise types compared with multi-condition training.

We note that our experiments did not use layer-wise pre-training not uncommonly used for small
datasets due to the difficulties of combining it with the invariance training. Our preliminary
experiments on a pre-trained network when using all noise types (last row of Table~\ref{tab:results}) for training show modest gains in WER over the multi-condition baseline. 

\begin{table}
    \centering
    \caption{Average word error rate (WER\%) on Aurora-4 dataset on all test conditions,
        including seen and unseen noise and unseen microphone. First column
        is the number of noise conditions used for the training. The last row is a 
        preliminary experiment with layer-wise pretraining close to state-of-the-art
        model and a corresponding invariance training starting with a pretrained model.}
    \label{tab:results}
    \begin{tabular}{r|cc||cc|cc|cc|cc}
    %\multirow{2}{*}{Subset} & \multicolumn{2}{c}{Orig}  & \multicolumn{2}{|c}{Inv}\\ 
    %           & wv1  & wv2  & wv1  & wv2 \\
        Noise       &Inv&BL&  \multicolumn{2}{c|}{A} & \multicolumn{2}{c|}{B} & \multicolumn{2}{c|}{C} & \multicolumn{2}{c}{D}\\
               & & &  Inv & BL & Inv & BL & Inv & BL & Inv & BL\\
    \hline
    1           &16.36        &18.14 &6.54&7.57    &12.71& 14.09   & 11.45&   13.10    & 22.47 &   24.80    \\
    2           &15.56        &17.39 &5.90&  6.58 &   11.69   &13.28   &11.12   &13.51   &21.79   &23.96 \\
    3           &14.24        &14.67 &5.45 & 5.08&    10.76&   12.44&   9.75&    9.84 &   19.93&   19.30\\
    4           &13.61        &13.84 & 5.08 &5.29    &9.73    &9.97    &9.49    &9.56    &19.49   &19.90\\         
    5           &13.41        &13.02 & 5.12 &5.34    &9.52    &9.42    &9.55    &8.67    &19.33   &18.65\\         
    6           &12.62        &12.60 & 4.80 &4.61    &9.04    &8.86    &8.76    &8.59    &18.16   &18.21\\
    \hline\hline
    6* &11.88        &11.99
    \end{tabular}
    %TODO: add numbers!
\end{table}

\section{Discussion}
    This paper presents the application of generative adversarial networks and invariance training for noise robust speech recognition. We show that invariance training helps the ASR system to generalize to better to unseen noise
    conditions and improves word error rate in when a small number of noise types are seen during training. Future
    research on this topic includes experimentation with layer-wise pre-training,
    exploring alternative network architectures and invariance-promoting loss functions
        
    %{\bf We need to make the message of this paragraph more clear.}
    % Dima: tried to fix -- it is differnt => needs more investigation
    Our experiments show that in contrast to the image recognition task,
    the task of speech recognition,  the domain adaptation network suffers from
    the underfitting. Therefore, the gradient of the $L_3$ term in Eq.~\ref{eq:cost}
    unreliable and noisy. This aspect needs further development.


\section*{Acknowledgments}

We would like to thank Yaroslav Ganin, David Warde-Farley for insightful discussions,
developers of Theano~\cite{2016arXiv160502688short}, Blocks, and Fuel~\cite{MerrienboerBDSW15} 
for great toolkits.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
%\bibliographystyle{IEEE}
\bibliographystyle{authordate1}
\bibliography{refs}

\end{document}
